[English](README.md) Â· [Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©](i18n/README.ar.md) Â· [EspaÃ±ol](i18n/README.es.md) Â· [FranÃ§ais](i18n/README.fr.md) Â· [æ—¥æœ¬èª](i18n/README.ja.md) Â· [í•œêµ­ì–´](i18n/README.ko.md) Â· [Tiáº¿ng Viá»‡t](i18n/README.vi.md) Â· [ä¸­æ–‡ (ç®€ä½“)](i18n/README.zh-Hans.md) Â· [ä¸­æ–‡ï¼ˆç¹é«”ï¼‰](i18n/README.zh-Hant.md) Â· [Deutsch](i18n/README.de.md) Â· [Ğ ÑƒÑÑĞºĞ¸Ğ¹](i18n/README.ru.md)


[![LazyingArt banner](https://github.com/lachlanchen/lachlanchen/raw/main/figs/banner.png)](https://github.com/lachlanchen/lachlanchen/blob/main/figs/banner.png)

# LazyLanguageLearner

[![License: Apache-2.0](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](LICENSE)
![Python](https://img.shields.io/badge/Python-3.10%2B-3776AB?logo=python&logoColor=white)
![Status](https://img.shields.io/badge/Status-Script--Driven-orange)
![Web](https://img.shields.io/badge/Web-Tornado-5C2D91?logo=tornado)
![AI](https://img.shields.io/badge/OpenAI-API-10A37F?logo=openai&logoColor=white)

| Attribute | Value |
|---|---|
| Type | Script-driven multilingual language learning pipeline |
| Runtime | Python CLI + Tornado web app |
| Main source | Rosetta Stone course PDFs |
| Storage | Local CSV + JSON cache files |
| Default port | `7788` |

LazyLanguageLearner is a script-driven Python workflow for turning language-course PDFs into reusable multilingual learning data and rendering it in a minimal web UI.

## ğŸŒ Overview

The repository combines content extraction, transformation, and serving:

| Step | Purpose |
|---|---|
| 1 | Download Rosetta Stone course-content PDFs from links embedded in `rs_html.py`. |
| 2 | Parse PDFs into sentence-level CSV rows for transformation. |
| 3 | Generate multilingual/phonetic variants through OpenAI with on-disk caching. |
| 4 | Render the structured sentences in a Tornado web UI with phonetic annotations. |

This project is intentionally lightweight and root-centric: scripts are designed to be run directly from the repository root rather than as an installed package.

## âœ¨ Features

- **Automated download workflow** from embedded links in `rs_html.py` using `download_course_text.py`.
- **Regex + PDF extraction pipeline** for section/sentence extraction in `pdf_to_csv.py`.
- **Selective extraction utilities** for level, section, page, and sentence inspection in `language_extraction.py`.
- **OpenAI request layer** (`openai_request.py`) with cache lookups, prompt handling, and basic JSON extraction retries.
- **Cross-language rendering pipeline** served by `app.py` and `templates/index.html`.
- **Japanese phonetic normalization** converting katakana data to hiragana before rendering.
- **On-disk caching** under `cache/` for generated translation requests and responses.

## ğŸ—‚ï¸ Project Structure

```text
.
â”œâ”€â”€ README.md
â”œâ”€â”€ LICENSE
â”œâ”€â”€ app.py
â”œâ”€â”€ download_course_text.py
â”œâ”€â”€ rs_html.py
â”œâ”€â”€ pdf_extraction.py
â”œâ”€â”€ pdf_to_csv.py
â”œâ”€â”€ language_extraction.py
â”œâ”€â”€ openai_request.py
â”œâ”€â”€ multilingual_sentence.py
â”œâ”€â”€ translations.json
â”œâ”€â”€ japanese_language_data.csv
â”œâ”€â”€ japanese_language_data copy.csv
â”œâ”€â”€ processed_words.csv
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ index.html
â”œâ”€â”€ cache/
â”‚   â””â”€â”€ *.json
â”œâ”€â”€ i18n/
â”‚   â””â”€â”€ README.*.md
â””â”€â”€ *.ipynb
```

## âœ… Prerequisites

- Python `3.10+`
- `pip` with an active virtual environment (`venv` recommended)
- An OpenAI API key (`OPENAI_API_KEY`) when using AI-powered generation
- A working internet connection for PDF downloads and OpenAI requests

Because there is no lockfile in this repo, dependencies are inferred from imports and previous content:

| Package | Used by |
|---|---|
| `tornado` | `app.py` |
| `openai` | `openai_request.py`, `multilingual_sentence.py` |
| `PyPDF2` | `pdf_to_csv.py`, `language_extraction.py` |
| `requests` | `download_course_text.py` |
| `beautifulsoup4` | `download_course_text.py` |

## ğŸ› ï¸ Installation

```bash
python3 -m venv .venv
source .venv/bin/activate
pip install --upgrade pip
pip install tornado openai PyPDF2 requests beautifulsoup4
```

| Setup hint | Command |
|---|---|
| Activate venv | `source .venv/bin/activate` |
| Reproduce env | `pip install tornado openai PyPDF2 requests beautifulsoup4` |
| Run checks | `python -m pip check` |

## ğŸš€ Usage

Run scripts in this order for the standard pipeline:

### 1) Download source PDFs

```bash
python download_course_text.py
```

Downloads PDFs into `downloaded_pdfs/`.

### 2) Extract PDF content to CSV

```bash
python pdf_to_csv.py
```

Outputs `japanese_language_data.csv` by default.

### 3) Inspect a specific PDF slice (optional)

```bash
python language_extraction.py
```

Useful for validating specific `level`, `section`, `page`, and `sentence_num` paths before generating broader datasets.

### 4) Build multilingual sentence payloads (optional)

```bash
python multilingual_sentence.py
```

Current behavior notes for reliability:

- The current version processes only the first row due to an early `break`.
- Prompt generation references `japanese_text`, which currently appears inconsistent with the extracted CSV row variable and may fail.

### 5) Start the web app

```bash
python app.py
```

- Default Tornado port: `7788`
- URL: `http://localhost:7788/`
- Known mismatch to verify in logs: the startup print message currently references `http://localhost:8888`.

## âš™ï¸ Configuration

Environment variables expected by the runtime scripts:

| Variable | Required | Purpose | Default |
|---|---|---|---|
| `OPENAI_API_KEY` | Yes (AI flow only) | OpenAI authentication | N/A |
| `OPENAI_MODEL` | No | Model override in requests | `gpt-4-0125-preview` |

Runtime files/directories:

- `downloaded_pdfs/` â€” populated by `download_course_text.py`.
- `cache/` â€” stores cached OpenAI prompt/response payloads.
- `translations.json` â€” consumed by the Tornado UI.
- `templates/index.html` â€” browser rendering template.

Assumptions:

- The repository root is the intended working directory for all scripts.
- Translation cache can be regenerated safely if stale or missing.

## ğŸ§¾ Examples

### CSV format (`japanese_language_data.csv`)

```csv
Level,Unit,Section,Sentence No.,Content
```

### OpenAI translation payload shape (`translations.json`)

```json
{
  "ja": {
    "full": "...",
    "pairs": [
      {
        "part": "æ—¥",
        "phonetic": "ã²"
      }
    ]
  },
  "en": { "full": "...", "pairs": [] },
  "ar": { "full": "...", "pairs": [] },
  "zh": { "full": "...", "pairs": [] },
  "yue": { "full": "...", "pairs": [] }
}
```

### Minimal quick check

```bash
python app.py
python - <<'PY'
import json
with open('translations.json', encoding='utf-8') as f:
    print('Loaded', len(json.load(f)), 'language keys')
PY
```

## ğŸ§ª Development Notes

- The project is not packaged (`requirements.txt`, `pyproject.toml`, and CI not present).
- Scripts are script-first and meant to be edited and rerun during iteration.
- Notebook files appear exploratory and should be treated as research aids, not production pipelines.
- `i18n/README.*.md` already exists for multilingual documentation, with the top-level language nav block in this file acting as the shared entry point.

## ğŸ©º Troubleshooting

- `ModuleNotFoundError`: install all required packages in the active virtual environment.
- `OPENAI` auth error / empty responses: check that `OPENAI_API_KEY` is exported in your shell.
- `FileNotFoundError` for `downloaded_pdfs`: run `python download_course_text.py` first.
- OpenAI conversion issues: inspect `cache/*.json` and verify input payload format expected by `multilingual_sentence.py`.
- App URL confusion: browse `http://localhost:7788/` after launch.

## ğŸ›£ï¸ Roadmap

- Add a dependency manifest (`requirements.txt` or `pyproject.toml`) for reproducible installs.
- Remove the one-row `break` from `multilingual_sentence.py` and support full-batch multilingual generation.
- Fix prompt variable usage in `multilingual_sentence.py` and add output validation.
- Correct Tornado startup URL log to reflect port `7788`.
- Add CLI flags (language, level, source paths, output path).
- Introduce lightweight tests for extraction, retry/parse logic, and JSON schema validation.
- Expand contributor-facing docs in `i18n` language variants.

## ğŸ¤ Contributing

Contributions are welcome.

1. Fork the repository.
2. Create a feature branch.
3. Make a focused change and keep script workflows easy to reproduce.
4. Open a pull request with clear rationale and before/after behavior notes.

If you update extraction logic, include sample inputs and outputs in your PR description.

## ğŸ™ Acknowledgements

- Rosetta Stone course-content links in `rs_html.py` are the source of downloadable PDF corpus references.
- OpenAI APIs are used for multilingual generation and phonetic annotation experiments.

## â¤ï¸ Support

| Donate | PayPal | Stripe |
| --- | --- | --- |
| [![Donate](https://camo.githubusercontent.com/24a4914f0b42c6f435f9e101621f1e52535b02c225764b2f6cc99416926004b7/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f446f6e6174652d4c617a79696e674172742d3045413545393f7374796c653d666f722d7468652d6261646765266c6f676f3d6b6f2d6669266c6f676f436f6c6f723d7768697465)](https://chat.lazying.art/donate) | [![PayPal](https://camo.githubusercontent.com/d0f57e8b016517a4b06961b24d0ca87d62fdba16e18bbdb6aba28e978dc0ea21/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f50617950616c2d526f6e677a686f754368656e2d3030343537433f7374796c653d666f722d7468652d6261646765266c6f676f3d70617970616c266c6f676f436f6c6f723d7768697465)](https://paypal.me/RongzhouChen) | [![Stripe](https://camo.githubusercontent.com/1152dfe04b6943afe3a8d2953676749603fb9f95e24088c92c97a01a897b4942/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f5374726970652d446f6e6174652d3633354246463f7374796c653d666f722d7468652d6261646765266c6f676f3d737472697065266c6f676f436f6c6f723d7768697465)](https://buy.stripe.com/aFadR8gIaflgfQV6T4fw400) |

## ğŸ“„ License

This project is licensed under the Apache License 2.0. See [LICENSE](LICENSE).
